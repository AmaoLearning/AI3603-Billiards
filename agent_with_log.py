"""
agent.py - Agent å†³ç­–æ¨¡å—

å®šä¹‰ Agent åŸºç±»å’Œå…·ä½“å®ç°ï¼š
- Agent: åŸºç±»ï¼Œå®šä¹‰å†³ç­–æ¥å£
- BasicAgent: åŸºäºè´å¶æ–¯ä¼˜åŒ–çš„å‚è€ƒå®ç°
- NewAgent: å­¦ç”Ÿè‡ªå®šä¹‰å®ç°æ¨¡æ¿
- analyze_shot_for_reward: å‡»çƒç»“æœè¯„åˆ†å‡½æ•°
"""

import math
import pooltool as pt
import numpy as np
from pooltool.objects import PocketTableSpecs, Table, TableType
import copy
import os
from datetime import datetime
import random
import logging
import signal
import functools
# from poolagent.pool import Pool as CuetipEnv, State as CuetipState
# from poolagent import FunctionAgent

from bayes_opt import BayesianOptimization, SequentialDomainReductionTransformer
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import Matern
import torch
from utils import calculate_ghost_ball_params, evaluate_state, get_pockets
from train.train_fast import AimNet
from agents import Agent

logger = logging.getLogger("evaluate")

# ============ è¶…æ—¶å®‰å…¨æ¨¡æ‹Ÿæœºåˆ¶ ============
class SimulationTimeoutError(Exception):
    """ç‰©ç†æ¨¡æ‹Ÿè¶…æ—¶å¼‚å¸¸"""
    pass

def _timeout_handler(signum, frame):
    """è¶…æ—¶ä¿¡å·å¤„ç†å™¨"""
    raise SimulationTimeoutError("ç‰©ç†æ¨¡æ‹Ÿè¶…æ—¶")

def simulate_with_timeout(shot, timeout=3):
    """å¸¦è¶…æ—¶ä¿æŠ¤çš„ç‰©ç†æ¨¡æ‹Ÿ
    
    å‚æ•°ï¼š
        shot: pt.System å¯¹è±¡
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤3ç§’
    
    è¿”å›ï¼š
        bool: True è¡¨ç¤ºæ¨¡æ‹ŸæˆåŠŸï¼ŒFalse è¡¨ç¤ºè¶…æ—¶æˆ–å¤±è´¥
    
    è¯´æ˜ï¼š
        ä½¿ç”¨ signal.SIGALRM å®ç°è¶…æ—¶æœºåˆ¶ï¼ˆä»…æ”¯æŒ Unix/Linuxï¼‰
        è¶…æ—¶åè‡ªåŠ¨æ¢å¤ï¼Œä¸ä¼šå¯¼è‡´ç¨‹åºå¡æ­»
    """
    # è®¾ç½®è¶…æ—¶ä¿¡å·å¤„ç†å™¨
    old_handler = signal.signal(signal.SIGALRM, _timeout_handler)
    signal.alarm(timeout)  # è®¾ç½®è¶…æ—¶æ—¶é—´
    
    try:
        pt.simulate(shot, inplace=True)
        signal.alarm(0)  # å–æ¶ˆè¶…æ—¶
        return True
    except SimulationTimeoutError:
        print(f"[WARNING] ç‰©ç†æ¨¡æ‹Ÿè¶…æ—¶ï¼ˆ>{timeout}ç§’ï¼‰ï¼Œè·³è¿‡æ­¤æ¬¡æ¨¡æ‹Ÿ")
        return False
    except Exception as e:
        signal.alarm(0)  # å–æ¶ˆè¶…æ—¶
        raise e
    finally:
        signal.signal(signal.SIGALRM, old_handler)  # æ¢å¤åŸå¤„ç†å™¨

# ============================================



def analyze_shot_for_reward(shot: pt.System, last_state: dict, player_targets: list):
    """
    åˆ†æå‡»çƒç»“æœå¹¶è®¡ç®—å¥–åŠ±åˆ†æ•°ï¼ˆå®Œå…¨å¯¹é½å°çƒè§„åˆ™ï¼‰
    
    å‚æ•°ï¼š
        shot: å·²å®Œæˆç‰©ç†æ¨¡æ‹Ÿçš„ System å¯¹è±¡
        last_state: å‡»çƒå‰çš„çƒçŠ¶æ€ï¼Œ{ball_id: Ball}
        player_targets: å½“å‰ç©å®¶ç›®æ ‡çƒIDï¼Œ['1', '2', ...] æˆ– ['8']
    
    è¿”å›ï¼š
        float: å¥–åŠ±åˆ†æ•°
            +50/çƒï¼ˆå·±æ–¹è¿›çƒï¼‰, +100ï¼ˆåˆæ³•é»‘8ï¼‰, +10ï¼ˆåˆæ³•æ— è¿›çƒï¼‰
            -100ï¼ˆç™½çƒè¿›è¢‹ï¼‰, -500ï¼ˆéæ³•é»‘8/ç™½çƒ+é»‘8ï¼‰, -30ï¼ˆé¦–çƒ/ç¢°åº“çŠ¯è§„ï¼‰
    
    è§„åˆ™æ ¸å¿ƒï¼š
        - æ¸…å°å‰ï¼šplayer_targets = ['1'-'7'] æˆ– ['9'-'15']ï¼Œé»‘8ä¸å±äºä»»ä½•äºº
        - æ¸…å°åï¼šplayer_targets = ['8']ï¼Œé»‘8æˆä¸ºå”¯ä¸€ç›®æ ‡çƒ
    """
    
    # 1. åŸºæœ¬åˆ†æ
    new_pocketed = [bid for bid, b in shot.balls.items() if b.state.s == 4 and last_state[bid].state.s != 4]
    
    # æ ¹æ® player_targets åˆ¤æ–­è¿›çƒå½’å±ï¼ˆé»‘8åªæœ‰åœ¨æ¸…å°åæ‰ç®—å·±æ–¹çƒï¼‰
    own_pocketed = [bid for bid in new_pocketed if bid in player_targets]
    enemy_pocketed = [bid for bid in new_pocketed if bid not in player_targets and bid not in ["cue", "8"]]
    
    cue_pocketed = "cue" in new_pocketed
    eight_pocketed = "8" in new_pocketed

    # 2. åˆ†æé¦–çƒç¢°æ’ï¼ˆå®šä¹‰åˆæ³•çš„çƒIDé›†åˆï¼‰
    first_contact_ball_id = None
    foul_first_hit = False
    valid_ball_ids = {'1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15'}
    
    for e in shot.events:
        et = str(e.event_type).lower()
        ids = list(e.ids) if hasattr(e, 'ids') else []
        if ('cushion' not in et) and ('pocket' not in et) and ('cue' in ids):
            # è¿‡æ»¤æ‰ 'cue' å’Œéçƒå¯¹è±¡ï¼ˆå¦‚ 'cue stick'ï¼‰ï¼Œåªä¿ç•™åˆæ³•çš„çƒID
            other_ids = [i for i in ids if i != 'cue' and i in valid_ball_ids]
            if other_ids:
                first_contact_ball_id = other_ids[0]
                break
    
    # é¦–çƒçŠ¯è§„åˆ¤å®šï¼šå®Œå…¨å¯¹é½ player_targets
    if first_contact_ball_id is None:
        # æœªå‡»ä¸­ä»»ä½•çƒï¼ˆä½†è‹¥åªå‰©ç™½çƒå’Œé»‘8ä¸”å·²æ¸…å°ï¼Œåˆ™ä¸ç®—çŠ¯è§„ï¼‰
        if len(last_state) > 2 or player_targets != ['8']:
            foul_first_hit = True
    else:
        # é¦–æ¬¡å‡»æ‰“çš„çƒå¿…é¡»æ˜¯ player_targets ä¸­çš„çƒ
        if first_contact_ball_id not in player_targets:
            foul_first_hit = True
    
    # 3. åˆ†æç¢°åº“
    cue_hit_cushion = False
    target_hit_cushion = False
    foul_no_rail = False
    
    for e in shot.events:
        et = str(e.event_type).lower()
        ids = list(e.ids) if hasattr(e, 'ids') else []
        if 'cushion' in et:
            if 'cue' in ids:
                cue_hit_cushion = True
            if first_contact_ball_id is not None and first_contact_ball_id in ids:
                target_hit_cushion = True

    if len(new_pocketed) == 0 and first_contact_ball_id is not None and (not cue_hit_cushion) and (not target_hit_cushion):
        foul_no_rail = True
        
    # è®¡ç®—å¥–åŠ±åˆ†æ•°
    score = 0
    
    if cue_pocketed and eight_pocketed:
        score -= 500
    elif cue_pocketed:
        score -= 100
    elif eight_pocketed:
        is_targeting_eight_ball_legally = (len(player_targets) == 1 and player_targets[0] == "8")
        score += 150 if is_targeting_eight_ball_legally else -500
            
    if foul_first_hit:
        score -= 30
    if foul_no_rail:
        score -= 30
        
    score += len(own_pocketed) * 50
    score -= len(enemy_pocketed) * 20
    
    if score == 0 and not cue_pocketed and not eight_pocketed and not foul_first_hit and not foul_no_rail:
        score = 10
        
    return score


class BasicAgent(Agent):
    """åŸºäºè´å¶æ–¯ä¼˜åŒ–çš„æ™ºèƒ½ Agent"""
    
    def __init__(self, target_balls=None):
        """åˆå§‹åŒ– Agent
        
        å‚æ•°ï¼š
            target_balls: ä¿ç•™å‚æ•°ï¼Œæš‚æœªä½¿ç”¨
        """
        super().__init__()
        
        # æœç´¢ç©ºé—´
        self.pbounds = {
            'V0': (0.5, 8.0),
            'phi': (0, 360),
            'theta': (0, 90), 
            'a': (-0.5, 0.5),
            'b': (-0.5, 0.5)
        }
        
        # ä¼˜åŒ–å‚æ•°
        self.INITIAL_SEARCH = 20
        self.OPT_SEARCH = 10
        self.ALPHA = 1e-2
        
        # æ¨¡æ‹Ÿå™ªå£°ï¼ˆå¯è°ƒæ•´ä»¥æ”¹å˜è®­ç»ƒéš¾åº¦ï¼‰
        self.noise_std = {
            'V0': 0.1,
            'phi': 0.1,
            'theta': 0.1,
            'a': 0.003,
            'b': 0.003
        }
        self.enable_noise = False
        
        print("BasicAgent (è´å¶æ–¯ä¼˜åŒ–ç‰ˆ) å·²åˆå§‹åŒ–ã€‚")

    
    def _create_optimizer(self, reward_function, seed):
        """åˆ›å»ºè´å¶æ–¯ä¼˜åŒ–å™¨
        
        å‚æ•°ï¼š
            reward_function: ç›®æ ‡å‡½æ•°ï¼Œ(V0, phi, theta, a, b) -> score
            seed: éšæœºç§å­
        
        è¿”å›ï¼š
            BayesianOptimizationå¯¹è±¡
        """
        gpr = GaussianProcessRegressor(
            kernel=Matern(nu=2.5),
            alpha=self.ALPHA,
            n_restarts_optimizer=10,
            random_state=seed
        )
        
        bounds_transformer = SequentialDomainReductionTransformer(
            gamma_osc=0.8,
            gamma_pan=1.0
        )
        
        optimizer = BayesianOptimization(
            f=reward_function,
            pbounds=self.pbounds,
            random_state=seed,
            verbose=0,
            bounds_transformer=bounds_transformer
        )
        optimizer._gp = gpr
        
        return optimizer


    def decision(self, balls=None, my_targets=None, table=None):
        """ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–æœç´¢æœ€ä½³å‡»çƒå‚æ•°
        
        å‚æ•°ï¼š
            balls: çƒçŠ¶æ€å­—å…¸ï¼Œ{ball_id: Ball}
            my_targets: ç›®æ ‡çƒIDåˆ—è¡¨ï¼Œ['1', '2', ...]
            table: çƒæ¡Œå¯¹è±¡
        
        è¿”å›ï¼š
            dict: å‡»çƒåŠ¨ä½œ {'V0', 'phi', 'theta', 'a', 'b'}
                å¤±è´¥æ—¶è¿”å›éšæœºåŠ¨ä½œ
        """
        if balls is None:
            print(f"[BasicAgent] Agent decisionå‡½æ•°æœªæ”¶åˆ°ballså…³é”®ä¿¡æ¯ï¼Œä½¿ç”¨éšæœºåŠ¨ä½œã€‚")
            return self._random_action()
        try:
            
            # ä¿å­˜ä¸€ä¸ªå‡»çƒå‰çš„çŠ¶æ€å¿«ç…§ï¼Œç”¨äºå¯¹æ¯”
            last_state_snapshot = {bid: copy.deepcopy(ball) for bid, ball in balls.items()}

            remaining_own = [bid for bid in my_targets if balls[bid].state.s != 4]
            if len(remaining_own) == 0:
                my_targets = ["8"]
                logger.info("[BasicAgent] æˆ‘çš„ç›®æ ‡çƒå·²å…¨éƒ¨æ¸…ç©ºï¼Œè‡ªåŠ¨åˆ‡æ¢ç›®æ ‡ä¸ºï¼š8å·çƒ")

            # 1.åŠ¨æ€åˆ›å»º"å¥–åŠ±å‡½æ•°" (Wrapper)
            # è´å¶æ–¯ä¼˜åŒ–å™¨ä¼šè°ƒç”¨æ­¤å‡½æ•°ï¼Œå¹¶ä¼ å…¥å‚æ•°
            def reward_fn_wrapper(V0, phi, theta, a, b):
                # åˆ›å»ºä¸€ä¸ªç”¨äºæ¨¡æ‹Ÿçš„æ²™ç›’ç³»ç»Ÿ
                sim_balls = {bid: copy.deepcopy(ball) for bid, ball in balls.items()}
                sim_table = copy.deepcopy(table)
                cue = pt.Cue(cue_ball_id="cue")

                shot = pt.System(table=sim_table, balls=sim_balls, cue=cue)
                
                try:
                    if self.enable_noise:
                        V0_noisy = V0 + np.random.normal(0, self.noise_std['V0'])
                        phi_noisy = phi + np.random.normal(0, self.noise_std['phi'])
                        theta_noisy = theta + np.random.normal(0, self.noise_std['theta'])
                        a_noisy = a + np.random.normal(0, self.noise_std['a'])
                        b_noisy = b + np.random.normal(0, self.noise_std['b'])
                        
                        V0_noisy = np.clip(V0_noisy, 0.5, 8.0)
                        phi_noisy = phi_noisy % 360
                        theta_noisy = np.clip(theta_noisy, 0, 90)
                        a_noisy = np.clip(a_noisy, -0.5, 0.5)
                        b_noisy = np.clip(b_noisy, -0.5, 0.5)
                        
                        shot.cue.set_state(V0=V0_noisy, phi=phi_noisy, theta=theta_noisy, a=a_noisy, b=b_noisy)
                    else:
                        shot.cue.set_state(V0=V0, phi=phi, theta=theta, a=a, b=b)
                    
                    # å…³é”®ï¼šä½¿ç”¨å¸¦è¶…æ—¶ä¿æŠ¤çš„ç‰©ç†æ¨¡æ‹Ÿï¼ˆ3ç§’ä¸Šé™ï¼‰
                    if not simulate_with_timeout(shot, timeout=3):
                        return 0  # è¶…æ—¶æ˜¯ç‰©ç†å¼•æ“é—®é¢˜ï¼Œä¸æƒ©ç½šagent
                except Exception as e:
                    # æ¨¡æ‹Ÿå¤±è´¥ï¼Œç»™äºˆæå¤§æƒ©ç½š
                    return -500
                
                # ä½¿ç”¨æˆ‘ä»¬çš„"è£åˆ¤"æ¥æ‰“åˆ†
                score = analyze_shot_for_reward(
                    shot=shot,
                    last_state=last_state_snapshot,
                    player_targets=my_targets
                )


                return score

            logger.info(f"[BasicAgent] æ­£åœ¨ä¸º Player (targets: {my_targets}) æœç´¢æœ€ä½³å‡»çƒ...")
            
            seed = np.random.randint(1e6)
            optimizer = self._create_optimizer(reward_fn_wrapper, seed)
            optimizer.maximize(
                init_points=self.INITIAL_SEARCH,
                n_iter=self.OPT_SEARCH
            )
            
            best_result = optimizer.max
            best_params = best_result['params']
            best_score = best_result['target']

            if best_score < 10:
                logger.info(f"[BasicAgent] æœªæ‰¾åˆ°å¥½çš„æ–¹æ¡ˆ (æœ€é«˜åˆ†: {best_score:.2f})ã€‚ä½¿ç”¨éšæœºåŠ¨ä½œã€‚")
                return self._random_action()
            action = {
                'V0': float(best_params['V0']),
                'phi': float(best_params['phi']),
                'theta': float(best_params['theta']),
                'a': float(best_params['a']),
                'b': float(best_params['b']),
            }

            logger.info(f"[BasicAgent] å†³ç­– (å¾—åˆ†: {best_score:.2f}): "
                  f"V0={action['V0']:.2f}, phi={action['phi']:.2f}, "
                  f"Î¸={action['theta']:.2f}, a={action['a']:.3f}, b={action['b']:.3f}")
            return action

        except Exception as e:
            logger.info(f"[BasicAgent] å†³ç­–æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯ï¼Œä½¿ç”¨éšæœºåŠ¨ä½œã€‚åŸå› : {e}")
            import traceback
            traceback.print_exc()
            return self._random_action()

# ============ BasicAgentPro: åŸºäºMCTSçš„è¿›é˜¶ Agent ============
class BasicAgentPro(Agent):
    """åŸºäºMCTSï¼ˆè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼‰çš„è¿›é˜¶ Agent"""
    
    def __init__(self,
                 n_simulations=50,       # ä»¿çœŸæ¬¡æ•°
                 c_puct=1.414):          # æ¢ç´¢ç³»æ•°
        super().__init__()
        self.n_simulations = n_simulations
        self.c_puct = c_puct
        self.ball_radius = 0.028575
        
        # å®šä¹‰å™ªå£°æ°´å¹³ (ä¸ poolenv ä¿æŒä¸€è‡´æˆ–ç•¥å¤§)
        self.sim_noise = {
            'V0': 0.1, 'phi': 0.15, 'theta': 0.1, 'a': 0.005, 'b': 0.005
        }
        
        logger.info("BasicAgentPro (MCTSç‰ˆ) å·²åˆå§‹åŒ–ã€‚")

    def _calc_angle_degrees(self, v):
        angle = math.degrees(math.atan2(v[1], v[0]))
        return angle % 360

    def _get_ghost_ball_target(self, cue_pos, obj_pos, pocket_pos):
        vec_obj_to_pocket = np.array(pocket_pos) - np.array(obj_pos)
        dist_obj_to_pocket = np.linalg.norm(vec_obj_to_pocket)
        if dist_obj_to_pocket == 0: return 0, 0
        unit_vec = vec_obj_to_pocket / dist_obj_to_pocket
        ghost_pos = np.array(obj_pos) - unit_vec * (2 * self.ball_radius)
        vec_cue_to_ghost = ghost_pos - np.array(cue_pos)
        dist_cue_to_ghost = np.linalg.norm(vec_cue_to_ghost)
        phi = self._calc_angle_degrees(vec_cue_to_ghost)
        return phi, dist_cue_to_ghost

    def generate_heuristic_actions(self, balls, my_targets, table):
        """
        ç”Ÿæˆå€™é€‰åŠ¨ä½œåˆ—è¡¨
        """
        actions = []
        
        cue_ball = balls.get('cue')
        if not cue_ball: return [self._random_action()]
        cue_pos = cue_ball.state.rvw[0]

        # è·å–æ‰€æœ‰ç›®æ ‡çƒçš„ID
        target_ids = [bid for bid in my_targets if balls[bid].state.s != 4]
        
        # å¦‚æœæ²¡æœ‰ç›®æ ‡çƒäº†ï¼ˆç†è®ºä¸Šå¤–éƒ¨ä¼šå¤„ç†è½¬ä¸º8å·ï¼Œè¿™é‡Œå…œåº•ï¼‰
        if not target_ids:
            target_ids = ['8']
        
        logger.info("[BasicAgentPro] æ­£åœ¨ä¸º Player (targets: %s) æœç´¢æœ€ä½³å‡»çƒ...", target_ids)

        # éå†æ¯ä¸€ä¸ªç›®æ ‡çƒ
        for tid in target_ids:
            obj_ball = balls[tid]
            obj_pos = obj_ball.state.rvw[0]

            # éå†æ¯ä¸€ä¸ªè¢‹å£
            for pocket_id, pocket in table.pockets.items():
                pocket_pos = pocket.center

                # 1. è®¡ç®—ç†è®ºè¿›çƒè§’åº¦
                phi_ideal, dist = self._get_ghost_ball_target(cue_pos, obj_pos, pocket_pos)

                # 2. æ ¹æ®è·ç¦»ç®€å•çš„ä¼°ç®—åŠ›åº¦ (è·ç¦»è¶Šè¿œåŠ›åº¦è¶Šå¤§ï¼ŒåŸºç¡€åŠ›åº¦2.0)
                v_base = 1.5 + dist * 1.5
                v_base = np.clip(v_base, 1.0, 6.0)

                # 3. ç”Ÿæˆå‡ ä¸ªå˜ç§åŠ¨ä½œåŠ å…¥å€™é€‰æ± 
                # å˜ç§1ï¼šç²¾å‡†ä¸€å‡»
                actions.append({
                    'V0': v_base, 'phi': phi_ideal, 'theta': 0, 'a': 0, 'b': 0
                })
                # å˜ç§2ï¼šåŠ›åº¦ç¨å¤§
                actions.append({
                    'V0': min(v_base + 1.5, 7.5), 'phi': phi_ideal, 'theta': 0, 'a': 0, 'b': 0
                })
                # å˜ç§3ï¼šè§’åº¦å¾®è°ƒ (å·¦å³åç§» 0.5 åº¦ï¼Œåº”å¯¹å™ªå£°)
                actions.append({
                    'V0': v_base, 'phi': (phi_ideal + 0.5) % 360, 'theta': 0, 'a': 0, 'b': 0
                })
                actions.append({
                    'V0': v_base, 'phi': (phi_ideal - 0.5) % 360, 'theta': 0, 'a': 0, 'b': 0
                })

        # å¦‚æœé€šè¿‡å¯å‘å¼æ²¡æœ‰ç”Ÿæˆä»»ä½•åŠ¨ä½œï¼ˆæç½•è§ï¼‰ï¼Œè¡¥å……éšæœºåŠ¨ä½œ
        if len(actions) == 0:
            for _ in range(5):
                actions.append(self._random_action())
        
        # éšæœºæ‰“ä¹±é¡ºåº
        random.shuffle(actions)
        return actions[:30]

    def simulate_action(self, balls, table, action):
        """
        [ä¿®æ”¹ç‚¹1] æ‰§è¡Œå¸¦å™ªå£°çš„ç‰©ç†ä»¿çœŸ
        è®© Agent æ„è¯†åˆ°ç”±äºè¯¯å·®çš„å­˜åœ¨ï¼ŒæŸäº›"æé™çƒ"æ˜¯ä¸å¯æ‰“çš„
        """
        sim_balls = {bid: copy.deepcopy(ball) for bid, ball in balls.items()}
        sim_table = copy.deepcopy(table)
        cue = pt.Cue(cue_ball_id="cue")
        shot = pt.System(table=sim_table, balls=sim_balls, cue=cue)
        
        try:
            # --- æ³¨å…¥é«˜æ–¯å™ªå£° ---
            noisy_V0 = np.clip(action['V0'] + np.random.normal(0, self.sim_noise['V0']), 0.5, 8.0)
            noisy_phi = (action['phi'] + np.random.normal(0, self.sim_noise['phi'])) % 360
            noisy_theta = np.clip(action['theta'] + np.random.normal(0, self.sim_noise['theta']), 0, 90)
            noisy_a = np.clip(action['a'] + np.random.normal(0, self.sim_noise['a']), -0.5, 0.5)
            noisy_b = np.clip(action['b'] + np.random.normal(0, self.sim_noise['b']), -0.5, 0.5)

            cue.set_state(V0=noisy_V0, phi=noisy_phi, theta=noisy_theta, a=noisy_a, b=noisy_b)
            pt.simulate(shot, inplace=True)
            return shot
        except Exception:
            return None

    def decision(self, balls=None, my_targets=None, table=None):
        if balls is None: return self._random_action()
        
        # é¢„å¤„ç†
        remaining = [bid for bid in my_targets if balls[bid].state.s != 4]
        if len(remaining) == 0: my_targets = ["8"]
        last_state_snapshot = {bid: copy.deepcopy(ball) for bid, ball in balls.items()}

        # ç”Ÿæˆå€™é€‰åŠ¨ä½œ
        candidate_actions = self.generate_heuristic_actions(balls, my_targets, table)
        n_candidates = len(candidate_actions)
        
        N = np.zeros(n_candidates)
        Q = np.zeros(n_candidates)
        
        # MCTS å¾ªç¯
        for i in range(self.n_simulations):
            # Selection (UCB)
            if i < n_candidates:
                idx = i
            else:
                total_n = np.sum(N)
                # ä½¿ç”¨å½’ä¸€åŒ–åçš„ Q è¿›è¡Œè®¡ç®—
                ucb_values = (Q / (N + 1e-6)) + self.c_puct * np.sqrt(np.log(total_n + 1) / (N + 1e-6))
                idx = np.argmax(ucb_values)
            
            # Simulation (å¸¦å™ªå£°)
            shot = self.simulate_action(balls, table, candidate_actions[idx])

            # Evaluation
            if shot is None:
                raw_reward = -500.0
            else:
                raw_reward = analyze_shot_for_reward(shot, last_state_snapshot, my_targets)
            
            # æ˜ å°„å…¬å¼: (val - min) / (max - min)
            normalized_reward = (raw_reward - (-500)) / 650.0
            # æˆªæ–­ä¸€ä¸‹é˜²æ­¢è¶Šç•Œ
            normalized_reward = np.clip(normalized_reward, 0.0, 1.0)

            # Backpropagation
            N[idx] += 1
            Q[idx] += normalized_reward # ç´¯åŠ å½’ä¸€åŒ–åçš„åˆ†æ•°

        # Final Decision
        # é€‰å¹³å‡åˆ†æœ€é«˜çš„ (Robust Child)
        avg_rewards = Q / (N + 1e-6)
        best_idx = np.argmax(avg_rewards)
        best_action = candidate_actions[best_idx]
        
        # ç®€å•æ‰“å°ä¸€ä¸‹å½“å‰æœ€å¥½çš„é¢„æµ‹èƒœç‡
        logger.info(f"[BasicAgentPro] Best Avg Score: {avg_rewards[best_idx]:.3f} (Sims: {self.n_simulations})")
        
        return best_action

class NewAgent(Agent):
    """è‡ªå®šä¹‰ Agent æ¨¡æ¿ï¼ˆå¾…å­¦ç”Ÿå®ç°ï¼‰"""
    
    def __init__(self):
        model_path = os.path.join('train', 'checkpoints', 'aim_model.pth')
        #self.agent = HybridLearningAgent(model_path)
        # self.agent = MCTSAgent()
        self.agent = BayesMCTSAgent(True)
    
    def decision(self, balls=None, my_targets=None, table=None):
        """å†³ç­–æ–¹æ³•
        
        å‚æ•°ï¼š
            observation: (balls, my_targets, table)
        
        è¿”å›ï¼š
            dict: {'V0', 'phi', 'theta', 'a', 'b'}
        """
        return self.agent.decision(balls=balls, my_targets=my_targets, table=table)
    
    def method(self):
        """
        å®é™…æ‰€ç”¨æ–¹æ³•çš„åç§°
        """
        return self.agent.__class__.__name__

class MCTSAgent(Agent):
    """
    MCTS-Lite Agent
    ç‰¹ç‚¹ï¼š
    1. å‡ ä½•æ±‚è§£ + å¾®è°ƒæ¨¡æ‹Ÿ
    2. åœ¨è¯„ä¼°å†³ç­–æ—¶å¤šçœ‹ä¸€æ­¥
    3. æ²¡æœ‰å­¦ä¹ è¿‡ç¨‹, æ¯”è¾ƒçœæ—¶

    æˆç»©: 29.0/40.0, 0.725
          æ¢ç”¨analyze_shot_for_rewardåä¸º 32.0/40.0, 0.800
          èµ°ä½è¯„åˆ†ç‰ˆæœ¬1 25.0/40.0, 0.625
          å®Œå…¨ä¸æˆªæ–­ç‰ˆæœ¬ 22.0/40.0, 0.550
          è§’åº¦æ”¾å®½åˆ°90/candidatesæ”¾å¤šåˆ°6ä¸ªç‰ˆæœ¬ 20.0/40.0, 0.500
    """
    def __init__(self):
        super().__init__()
        logger.info("ImprovedMCTSAgent å·²åˆå§‹åŒ– - åŒ…å«é˜²å®ˆé€»è¾‘ä¸å¾®è°ƒç„å‡†")

    def _generate_safety_shot(self, balls, candidates):
        """
        é˜²å®ˆç­–ç•¥ï¼šå½“æ²¡æœ‰å¥½æœºä¼šæ—¶ï¼Œè½»è½»ç¢°ä¸€ä¸‹ç¦»å¾—æœ€è¿‘çš„çƒï¼Œé¿å…çŠ¯è§„
        """
        logger.info("[MCTSAgent] å¯åŠ¨é˜²å®ˆæ¨¡å¼ (Safety Mode)")
        cue_pos = balls['cue'].state.rvw[0]
        min_dist = float('inf')
        best_target = None
        
        # æ‰¾æœ€è¿‘çš„è‡ªå·±çš„çƒ
        for bid in candidates:
            obj_pos = balls[bid].state.rvw[0]
            dist = np.linalg.norm(np.array(obj_pos[:2]) - np.array(cue_pos[:2]))
            if dist < min_dist:
                min_dist = dist
                best_target = bid
        
        if best_target:
            obj_pos = balls[best_target].state.rvw[0]
            dx = obj_pos[0] - cue_pos[0]
            dy = obj_pos[1] - cue_pos[1]
            phi = np.degrees(np.arctan2(dy, dx)) % 360
            # æè½»çš„åŠ›åº¦ï¼Œåªè¦ç¢°åˆ°å°±è¡Œ
            logger.info(f'[MCTSAgent] è½»è½»è§¦ç¢°å·±æ–¹çƒ {best_target}')
            return {'V0': 0.8, 'phi': phi, 'theta': 0, 'a': 0, 'b': 0}
        
        return self._random_action()

    def decision(self, balls=None, my_targets=None, table=None):
        if balls is None:
            return self._random_action()
        
        cue_ball = balls['cue']
        cue_pos = cue_ball.state.rvw[0]
        R = cue_ball.params.R

        last_state_snapshot = {bid: copy.deepcopy(ball) for bid, ball in balls.items()}
        # --- 1. ç”Ÿæˆå‡ ä½•å€™é€‰ (Candidates) ---
        candidates = [] 
        remaining_own = [bid for bid in my_targets if balls[bid].state.s != 4]
        if not remaining_own: remaining_own = ['8']

        logger.info("[MCTSAgent] æ­£åœ¨ä¸º Player (targets: %s) æœç´¢æœ€ä½³å‡»çƒ...", remaining_own)

        for ball_id in remaining_own:
            obj_pos = balls[ball_id].state.rvw[0]
            for pid, pocket in table.pockets.items():
                phi_ideal, cut_angle, dist = calculate_ghost_ball_params(cue_pos, obj_pos, pocket.center, R)
                
                # åªæœ‰éå¸¸éš¾æ‰“çš„çƒæ‰ä¼šè¢«è¿‡æ»¤ (é˜ˆå€¼ 85åº¦)
                if abs(cut_angle) > 85: continue
                
                candidates.append({
                    'target_id': ball_id,
                    'phi_center': phi_ideal,
                    'cut_angle': cut_angle,
                    'distance': dist
                })

        # å¦‚æœçœŸçš„æ²¡æœ‰è¿›æ”»æœºä¼šï¼Œæ‰§è¡Œé˜²å®ˆ
        if not candidates:
            logger.info("[MCTSAgent] æ— å‡ ä½•è¿›æ”»çº¿è·¯ï¼Œå°è¯•é˜²å®ˆã€‚")
            return self._generate_safety_shot(balls, remaining_own)

        # æ’åºï¼šä¼˜å…ˆè€ƒè™‘åˆ‡è§’å°ã€è·ç¦»è¿‘çš„çƒ
        candidates.sort(key=lambda x: x['cut_angle'] + x['distance']*10)
        top_candidates = candidates[:4] # åªçœ‹å‰4ä¸ªæœ€å¥½çš„é€‰æ‹©

        best_action = None
        best_score = -float('inf')
        best_tag = ""

        # --- 2. æ¨¡æ‹Ÿä¸å¾®è°ƒ (Simulation) ---
        logger.info("[MCTSAgent] è¯„ä¼° %d ä¸ªè¿›æ”»çº¿è·¯...", len(top_candidates))
        
        for cand in top_candidates: # åˆ°åªå‰©8æ—¶ç»å¸¸åªæœ‰1ä¸ªcandï¼Œè¿™è¯´æ˜ç™½çƒä½ç½®ä¸å¥½
            # å¾®è°ƒé€»è¾‘ï¼šä¸ä»…å°è¯•ç†è®ºè§’åº¦ï¼Œè¿˜è¦å°è¯•å·¦å³åå·®
            # ç‰©ç†å¼•æ“ä¸­ï¼Œçƒçš„ç¢°æ’ä¼šæœ‰åå·®ï¼Œå¿…é¡»é€šè¿‡å¾®è°ƒæ¥ä¿®æ­£
            phi_offsets = [0, -0.5, 0.5, -1.0, 1.0] 
            speeds = [0.8, 2.0, 4.0, 6.5] # è½»æ¨ã€æ…¢ã€ä¸­ã€å¿«
            
            for V0 in speeds:
                for offset in phi_offsets:
                    phi_try = cand['phi_center'] + offset
                    
                    # æ„å»ºæ¨¡æ‹Ÿç¯å¢ƒ
                    sim_balls = {bid: copy.deepcopy(ball) for bid, ball in balls.items()}
                    sim_table = copy.deepcopy(table)
                    cue = pt.Cue(cue_ball_id="cue")
                    cue.set_state(V0=V0, phi=phi_try, theta=0, a=0, b=0)
                    shot = pt.System(table=sim_table, balls=sim_balls, cue=cue)
                    
                    try:
                        pt.simulate(shot, inplace=True)
                        score = evaluate_state(shot, last_state_snapshot, my_targets)
                        
                        if score > best_score:
                            best_score = score
                            best_action = {'V0': V0, 'phi': phi_try, 'theta': 0, 'a': 0, 'b': 0}
                            # å¦‚æœæ‰¾åˆ°äº†å¿…è¿›çƒä¸”èµ°ä½ä¸é”™çš„è§£ï¼Œå¯ä»¥æå‰å‰ªæï¼Œä¹Ÿç›¸å½“äºå€¾å‘äºé€Ÿåº¦æ›´å°çš„åŠ¨ä½œ
                            if score >= 60: 
                                logger.info("[MCTSAgent] æ‰¾åˆ°ç»ä½³çº¿è·¯ï¼å†³ç­–: V0=%.1f, phi=%.1f (ExpScore:%.1f)",
                                                best_action['V0'],
                                                best_action['phi'],
                                                best_score,)
                                return best_action
                                
                    except Exception as e:
                        # æ‰“å°é”™è¯¯ä½†ä¸ä¸­æ–­ç¨‹åº
                        logger.error("[MCTSAgent ERROR] Sim failed: %s", e)
                        continue

        if (best_action is None) or (best_score < 0): # æ›´æ¿€è¿›åœ°é‡‡å–é˜²å®ˆç­–ç•¥
            logger.info("[MCTSAgent] æ¨¡æ‹Ÿåæœªå‘ç°å¯è¡Œè¿›æ”»æ–¹æ¡ˆï¼Œè½¬ä¸ºé˜²å®ˆã€‚")
            return self._generate_safety_shot(balls, remaining_own)
            
        logger.info(
            "[MCTSAgent] å†³ç­–: V0=%.1f, phi=%.1f (ExpScore:%.1f)",
            best_action['V0'],
            best_action['phi'],
            best_score,
        )
        return best_action

    def _random_action(self):
        return {'V0': 1.0, 'phi': np.random.uniform(0,360), 'theta':0, 'a':0, 'b':0}

class BankAgent(Agent):
    """
    BankAgent (MCTS + Bank Shots)
    ç‰¹ç‚¹ï¼š
    1. ç»§æ‰¿äº† MCTSAgent çš„å‡ ä½•æ±‚è§£ + å¾®è°ƒæ¨¡æ‹Ÿèƒ½åŠ›
    2. æ–°å¢ï¼šç¿»è¢‹ (Bank Shot) è·¯å¾„è§„åˆ’èƒ½åŠ›
    3. èƒ½å¤Ÿè¯†åˆ«ç›´æ‰“å›°éš¾çš„å±€é¢ï¼Œè‡ªåŠ¨å¯»æ‰¾æ’åº“è§£æ³•
    """
    def __init__(self):
        super().__init__()
        logger.info("BankAgent å·²åˆå§‹åŒ– - å…·å¤‡ç¿»è¢‹æ”»å‡»èƒ½åŠ›")

    def _get_table_rails(self, table):
        """è·å–4ä¸ªåº“è¾¹çš„åæ ‡ä½ç½®"""
        # table.w æ˜¯å®½ (yè½´æ–¹å‘), table.l æ˜¯é•¿ (xè½´æ–¹å‘)
        # å‡è®¾çƒæ¡Œä¸­å¿ƒåœ¨ (0,0)
        right = table.l / 2
        left = -table.l / 2
        top = table.w / 2
        bottom = -table.w / 2
        return {'left': left, 'right': right, 'top': top, 'bottom': bottom}

    def _get_virtual_pocket(self, pocket_pos, rail_name, rails):
        """è®¡ç®—è™šæ‹Ÿè¢‹å£åæ ‡ (é•œåƒç‚¹)"""
        px, py = pocket_pos[0], pocket_pos[1]
        
        if rail_name == 'top':   # y = top, é•œåƒç‚¹ y' = 2*top - y
            return np.array([px, 2 * rails['top'] - py, 0])
        elif rail_name == 'bottom':
            return np.array([px, 2 * rails['bottom'] - py, 0])
        elif rail_name == 'left': # x = left, é•œåƒç‚¹ x' = 2*left - x
            return np.array([2 * rails['left'] - px, py, 0])
        elif rail_name == 'right':
            return np.array([2 * rails['right'] - px, py, 0])
        return None

    def _generate_safety_shot(self, balls, my_targets):
        """é˜²å®ˆç­–ç•¥"""
        logger.info("[BankAgent] å¯åŠ¨é˜²å®ˆæ¨¡å¼ (Safety Mode)")
        cue_pos = balls['cue'].state.rvw[0]
        min_dist = float('inf')
        best_target = None
        
        candidates = [b for b in my_targets if balls[b].state.s != 4]
        if not candidates: candidates = ['8']
        
        for bid in candidates:
            obj_pos = balls[bid].state.rvw[0]
            dist = np.linalg.norm(np.array(obj_pos[:2]) - np.array(cue_pos[:2]))
            if dist < min_dist:
                min_dist = dist
                best_target = bid
        
        if best_target:
            obj_pos = balls[best_target].state.rvw[0]
            dx = obj_pos[0] - cue_pos[0]
            dy = obj_pos[1] - cue_pos[1]
            phi = np.degrees(np.arctan2(dy, dx)) % 360
            return {'V0': 0.8, 'phi': phi, 'theta': 0, 'a': 0, 'b': 0}
        
        return self._random_action()

    def _generate_bank_candidates(self, balls, my_targets, table, cue_pos, R):
        """ç”Ÿæˆç¿»è¢‹å‡»çƒå€™é€‰"""
        candidates = []
        rails = self._get_table_rails(table)
        
        remaining_own = [bid for bid in my_targets if balls[bid].state.s != 4]
        if not remaining_own: remaining_own = ['8']

        for ball_id in remaining_own:
            obj_pos = balls[ball_id].state.rvw[0]
            
            # éå†4æ¡åº“è¾¹
            for rail_name in ['top', 'bottom', 'left', 'right']:
                for pid, pocket in table.pockets.items():
                    # è®¡ç®—è™šæ‹Ÿè¢‹å£
                    virtual_pos = self._get_virtual_pocket(pocket.center, rail_name, rails)
                    
                    # å‡ ä½•è®¡ç®—ï¼šæŠŠè™šæ‹Ÿè¢‹å£å½“åšç›®æ ‡
                    phi_ideal, cut_angle, dist_cue_obj = calculate_ghost_ball_params(
                        cue_pos, obj_pos, virtual_pos, R
                    )
                    
                    # ç¿»è¢‹éš¾åº¦è¾ƒå¤§ï¼Œåˆ‡è§’é˜ˆå€¼è®¾ä½ä¸€ç‚¹ (å¦‚ 60åº¦)
                    if abs(cut_angle) > 60: continue
                    
                    # ä¼°ç®—æ€»è·ç¦»ï¼šæ¯çƒ->ç›®æ ‡çƒ + ç›®æ ‡çƒ->è™šæ‹Ÿè¢‹å£
                    dist_obj_virtual = np.linalg.norm(virtual_pos - obj_pos)
                    total_dist = dist_cue_obj + dist_obj_virtual
                    
                    candidates.append({
                        'type': 'bank', # æ ‡è®°ç±»å‹
                        'target_id': ball_id,
                        'phi_center': phi_ideal,
                        'cut_angle': cut_angle,
                        'distance': total_dist,
                        'rail': rail_name,
                        'pocket_id': pid
                    })
        return candidates

    def decision(self, balls=None, my_targets=None, table=None):
        if balls is None: return self._random_action()
        
        cue_ball = balls['cue']
        cue_pos = cue_ball.state.rvw[0]
        R = cue_ball.params.R

        # --- 1. ç”Ÿæˆç›´æ‰“å€™é€‰ (Direct) ---
        candidates = [] 
        remaining_own = [bid for bid in my_targets if balls[bid].state.s != 4]
        if not remaining_own: remaining_own = ['8']

        for ball_id in remaining_own:
            obj_pos = balls[ball_id].state.rvw[0]
            for pid, pocket in table.pockets.items():
                phi_ideal, cut_angle, dist = calculate_ghost_ball_params(cue_pos, obj_pos, pocket.center, R)
                
                if abs(cut_angle) > 85: continue
                
                candidates.append({
                    'type': 'direct',
                    'target_id': ball_id,
                    'phi_center': phi_ideal,
                    'cut_angle': cut_angle,
                    'distance': dist
                })

        # --- 2. ç”Ÿæˆç¿»è¢‹å€™é€‰ (Bank) ---
        bank_candidates = self._generate_bank_candidates(balls, my_targets, table, cue_pos, R)
        candidates.extend(bank_candidates)

        if not candidates:
            logger.info("[BankAgent] æ— å‡ ä½•è¿›æ”»çº¿è·¯ï¼Œå°è¯•é˜²å®ˆã€‚")
            return self._generate_safety_shot(balls, my_targets)

        # --- 3. æ··åˆæ’åº ---
        # ç•¥å¾®é™ä½ç¿»è¢‹æƒ©ç½šï¼Œé¼“åŠ±åœ¨ç›´æ‰“å›°éš¾æ—¶å°è¯•ç¿»è¢‹
        def sort_key(c):
            penalty = 0 if c['type'] == 'direct' else 25
            return c['cut_angle'] + c['distance']*10 + penalty

        candidates.sort(key=sort_key)
        top_candidates = candidates[:5]  # åªå…³æ³¨å‰5ä¸ªæœºä¼š

        logger.info(
            "[BankAgent] è¯„ä¼° %d ä¸ªçº¿è·¯ (å« %d ä¸ªç¿»è¢‹)...",
            len(top_candidates),
            sum(1 for c in top_candidates if c['type']=='bank'),
        )

        best_action = None
        best_score = 0  # åªæ¥å—å¾—åˆ†>0çš„æ–¹æ¡ˆ
        best_tag = ""

        # --- 4. æ¨¡æ‹Ÿä¸é«˜ç²¾åº¦å¾®è°ƒ ---
        for cand in top_candidates:
            # é«˜å¯†åº¦è§’åº¦å¾®è°ƒï¼šç›´æ‰“[-1.5,1.5]åˆ†21ä»½ï¼›ç¿»è¢‹[-2.5,2.5]åˆ†31ä»½
            phi_offsets = np.linspace(-1.5, 1.5, 21)
            
            if cand['type'] == 'bank':
                phi_offsets = np.linspace(-2.5, 2.5, 31)
                base_speeds = [4.0, 6.0, 8.0]
            else:
                base_speeds = [2.5, 4.5, 6.5]
            
            for V0 in base_speeds:
                # å¦‚æœå·²æœ‰è¾ƒé«˜å¾—åˆ†ï¼Œå½“å‰çº¿è·¯ä¸å†å°è¯•æ›´å¤šåŠ›åº¦ä»¥èŠ‚çœæ—¶é—´
                if best_score > 80:
                    break

                for offset in phi_offsets:
                    phi_try = cand['phi_center'] + offset
                    
                    sim_balls = {bid: copy.deepcopy(ball) for bid, ball in balls.items()}
                    sim_table = copy.deepcopy(table)
                    cue = pt.Cue(cue_ball_id="cue")
                    cue.set_state(V0=V0, phi=phi_try, theta=0, a=0, b=0)
                    shot = pt.System(table=sim_table, balls=sim_balls, cue=cue)
                    
                    try:
                        pt.simulate(shot, inplace=True)
                        score = evaluate_state(shot, my_targets, cand['target_id'])
                        
                        if score > best_score:
                            best_score = score
                            best_action = {'V0': V0, 'phi': phi_try, 'theta': 0, 'a': 0, 'b': 0}
                            best_tag = "[ç¿»è¢‹]" if cand['type'] == 'bank' else "[ç›´æ‰“]"

                            if score > 120:
                                logger.info("[BankAgent] é”å®š%sç»ä½³çº¿è·¯ï¼Score: %.1f", best_tag, score)
                                return best_action
                    except Exception:
                        continue

        if best_action is None:
            logger.info("[BankAgent] æ¨¡æ‹Ÿæ˜¾ç¤ºæ— è¿›çƒæœºä¼š (BestScore: %.1f)ï¼Œæ™ºèƒ½è½¬ä¸ºé˜²å®ˆã€‚", best_score)
            return self._generate_safety_shot(balls, my_targets)
            
        logger.info(
            "[BankAgent] å†³ç­–: V0=%.1f, phi=%.1f, score=%.1f %s",
            best_action['V0'],
            best_action['phi'],
            best_score,
            best_tag,
        )
        return best_action

    def _random_action(self):
        return {'V0': 1.0, 'phi': np.random.uniform(0,360), 'theta':0, 'a':0, 'b':0}

class HybridLearningAgent(Agent):
    """
    æ··åˆæ™ºèƒ½ä½“ï¼šç¥ç»ç½‘ç»œå¼•å¯¼ + å±€éƒ¨æœç´¢éªŒè¯
    ç­–ç•¥ï¼š
    1. ä½¿ç”¨ç¥ç»ç½‘ç»œé¢„æµ‹åå·®ï¼Œå¤§å¹…ç¼©å°æœç´¢èŒƒå›´ã€‚
    2. åœ¨é¢„æµ‹å€¼é™„è¿‘è¿›è¡Œå¾®å°èŒƒå›´çš„æ¨¡æ‹ŸéªŒè¯ï¼ˆè§£å†³ç‰©ç†å™ªå£°ï¼‰ã€‚
    3. å¦‚æœè¿›æ”»æ¨¡æ‹Ÿå…¨éƒ¨å¤±è´¥ï¼Œä¸¥æ ¼æ‰§è¡Œé˜²å®ˆï¼ˆè§£å†³ä¹±æ‰“é—®é¢˜ï¼‰ã€‚
   
    æˆç»©ï¼šæ¢ç”¨analyze_shot_for_rewardå 23.0/40.0 0.575
    """
    def __init__(self, model_path='checkpoints/aim_model.pth'):
        super().__init__()
        self.model = AimNet()
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        try:
            # åŠ è½½æ¨¡å‹
            state_dict = torch.load(model_path, map_location=self.device)
            self.model.load_state_dict(state_dict)
            self.model.to(self.device)
            self.model.eval()
            logger.info(f"[HybridAgent] ç¥ç»ç½‘ç»œåŠ è½½æˆåŠŸ: {model_path}")
            self.use_nn = True
        except Exception as e:
            logger.info(f"[HybridAgent] æ¨¡å‹åŠ è½½å¤±è´¥ ({e})ï¼Œå›é€€åˆ°çº¯å‡ ä½•æœç´¢æ¨¡å¼ã€‚")
            self.use_nn = False

    def _predict_correction(self, cut_angle, distance, V0):
        """è°ƒç”¨ç¥ç»ç½‘ç»œé¢„æµ‹ä¿®æ­£é‡"""
        if not self.use_nn: return 0.0

        c_norm = cut_angle / 90.0
        d_norm = distance / 2.05
        v_norm = V0 / 8.0
        
        inputs = torch.tensor([[c_norm, d_norm, v_norm]], dtype=torch.float32).to(self.device)
        
        with torch.no_grad():
            delta = self.model(inputs).item()
        return delta

    def _generate_safety_shot(self, balls, my_targets):
        """é˜²å®ˆç­–ç•¥ï¼šå¿…é¡»ç¢°åº“"""
        logger.info("[HybridAgent] è¿›æ”»ä¸å¯è¡Œï¼Œåˆ‡æ¢å¼ºåŠ›é˜²å®ˆã€‚")
        cue_pos = balls['cue'].state.rvw[0]
        min_dist = float('inf')
        best_target = None
        
        candidates = [b for b in my_targets if balls[b].state.s != 4]
        if not candidates: candidates = ['8']
        
        # æ‰¾æœ€è¿‘çš„çƒ
        for bid in candidates:
            obj_pos = balls[bid].state.rvw[0]
            dist = np.linalg.norm(np.array(obj_pos[:2]) - np.array(cue_pos[:2]))
            if dist < min_dist:
                min_dist = dist
                best_target = bid
        
        if best_target:
            obj_pos = balls[best_target].state.rvw[0]
            dx = obj_pos[0] - cue_pos[0]
            dy = obj_pos[1] - cue_pos[1]
            phi = np.degrees(np.arctan2(dy, dx)) % 360
            # åŠ›åº¦ 3.0ï¼Œç¡®ä¿ç¢°åº“ä¸çŠ¯è§„
            return {'V0': 3.0, 'phi': phi, 'theta': 0, 'a': 0, 'b': 0}
        
        # å®åœ¨æ²¡åŠæ³•ï¼Œéšæœºæ‰“
        return {'V0': 1.0, 'phi': 0, 'theta':0, 'a':0, 'b':0}

    def decision(self, balls=None, my_targets=None, table=None):
        if balls is None: return self._generate_safety_shot(balls, my_targets)
        
        cue_ball = balls['cue']
        cue_pos = cue_ball.state.rvw[0]
        R = cue_ball.params.R

        
        last_state_snapshot = {bid: copy.deepcopy(ball) for bid, ball in balls.items()}
        # --- 1. ç­›é€‰å€™é€‰çƒ ---
        candidates = []
        remaining = [bid for bid in my_targets if balls[bid].state.s != 4]
        if not remaining: remaining = ['8']
        
        for ball_id in remaining:
            obj_pos = balls[ball_id].state.rvw[0]
            for pid, pocket in table.pockets.items():
                phi_geo, cut_angle, dist = calculate_ghost_ball_params(cue_pos, obj_pos, pocket.center, R)
                # ä¸¥æ ¼è¿‡æ»¤ > 85 åº¦çš„çƒ
                if abs(cut_angle) > 85: continue
                
                candidates.append({
                    'target_id': ball_id,
                    'phi_geo': phi_geo,
                    'cut_angle': cut_angle,
                    'distance': dist
                })
        
        # æ’åºï¼šä¼˜å…ˆåˆ‡è§’å°ã€è·ç¦»è¿‘
        candidates.sort(key=lambda x: x['cut_angle'] + x['distance']*10)
        top_candidates = candidates[:4] # åªçœ‹æœ€å¥½çš„4ä¸ª

        best_action = None
        best_score = 0 # ã€å…³é”®ä¿®å¤ã€‘: åˆå§‹åˆ†æ•°è®¾ä¸º0ï¼Œä»»ä½•è´Ÿåˆ†(æ²¡è¿›)éƒ½ä¸ä¼šè¢«é€‰ä¸­

        # --- 2. æ··åˆæœç´¢ (Neural Guide + Local Search) ---
        for cand in top_candidates:
            # å°è¯•å‡ ç§åŠ›åº¦
            speeds = [3.0, 5.0, 7.0]
            
            for V0 in speeds:
                # Step A: ç¥ç»ç½‘ç»œé¢„æµ‹åå·®
                delta_pred = self._predict_correction(cand['cut_angle'], cand['distance'], V0)
                phi_center = cand['phi_geo'] + delta_pred
                
                # Step B: å±€éƒ¨å¾®è°ƒ (Local Grid Search)
                # æ—¢ç„¶ç½‘ç»œå¯èƒ½æœ‰è¯¯å·®ï¼Œæˆ‘ä»¬åœ¨é¢„æµ‹å€¼å·¦å³ 0.5åº¦ å†…å†æ‰« 5 ä¸ªç‚¹
                # è¿™æ ·æ—¢åˆ©ç”¨äº†ç½‘ç»œçš„å…ˆéªŒï¼Œåˆè§£å†³äº†ç‰©ç†å™ªå£°
                offsets = np.linspace(-0.5, 0.5, 5) 
                
                for off in offsets:
                    phi_try = phi_center + off
                    
                    # æ¨¡æ‹Ÿ
                    sim_balls = {bid: copy.deepcopy(ball) for bid, ball in balls.items()}
                    sim_table = copy.deepcopy(table)
                    cue = pt.Cue(cue_ball_id="cue")
                    cue.set_state(V0=V0, phi=phi_try, theta=0, a=0, b=0)
                    shot = pt.System(table=sim_table, balls=sim_balls, cue=cue)
                    
                    try:
                        pt.simulate(shot, inplace=True)
                        score = evaluate_state(shot, last_state_snapshot, my_targets)
                        
                        # å¦‚æœæ˜¯å¥½ç»“æœ
                        if score > best_score:
                            best_score = score
                            best_action = {'V0': V0, 'phi': phi_try, 'theta': 0, 'a': 0, 'b': 0}
                            
                            # æå‰å‰ªæï¼šå¦‚æœå·²ç»ç¨³è¿›çƒäº†ï¼Œä¸ç”¨å†æœäº†
                            if score > 80: 
                                logger.info(f"[Hybrid] ğŸ¯ å‘½ä¸­! NNå:{delta_pred:.2f} | å¾®è°ƒ:{off:.2f} | Score:{score:.1f}")
                                return best_action
                                
                    except: continue

        # --- 3. å†³ç­– ---
        if best_action is not None and best_score > 0:
            logger.info(f"[Hybrid] å†³ç­–: V0={best_action['V0']:.1f}, phi={best_action['phi']:.1f} (ExpScore:{best_score:.1f})")
            return best_action
        
        # å¦‚æœæ¨¡æ‹Ÿäº†ä¸€åœˆï¼Œåˆ†æ•°å…¨æ˜¯ -50 æˆ– -1000ï¼Œè¯´æ˜æ ¹æœ¬è¿›ä¸å»
        # ã€å…³é”®ä¿®å¤ã€‘: ç»å¯¹ä¸é€‰é‚£äº› -50 çš„åŠ¨ä½œï¼Œè½¬ä¸ºé˜²å®ˆ
        logger.info(f"[Hybrid] è¿›æ”»æ¨¡æ‹Ÿå…¨å¤±è´¥ (BestScore: {best_score})ï¼Œè½¬ä¸ºé˜²å®ˆã€‚")
        return self._generate_safety_shot(balls, my_targets)

class BayesMCTSAgent(Agent):
    """åŸºäºè´å¶æ–¯ä¼˜åŒ–çš„æ™ºèƒ½MCTS Agent"""
    
    def __init__(self, enable_noise=False):
        """åˆå§‹åŒ– Agent
        
        å‚æ•°ï¼š
            target_balls: ä¿ç•™å‚æ•°ï¼Œæš‚æœªä½¿ç”¨
        
        æˆç»©ï¼š
            å•æ‰“8å·çƒ: 31.0/40.0
            analyze_shot_for_reward: 32.0/40.0
            evaluate_state: 28.0/40.0
            v1 vs BasicPro: 43.0/120.0
            v2 vs Basic: 100.0/120.0
            v2 with Noise vs Basic: 106.0/120.0
        """
        super().__init__()
        
        # æœç´¢ç©ºé—´ - æ‰©å¤§è§’åº¦æœç´¢èŒƒå›´ä»¥é€‚åº”åˆ‡çƒ
        self.pbounds = {            'd_V0': (-2.0, 2.0),
            'd_phi': (-3.0, 3.0),  # ä» Â±0.5 æ‰©å¤§åˆ° Â±3.0ï¼Œå…³é”®æ”¹è¿›ï¼
            'theta': (0, 30),      # é™åˆ¶è·³çƒè§’åº¦ï¼Œå‡å°‘æ— æ•ˆæœç´¢
            'a': (-0.2, 0.2),      # ç¼©å°å¡çƒèŒƒå›´ï¼Œå‡å°‘å¤æ‚åº¦
            'b': (-0.2, 0.2)
        }
        
        # ä¼˜åŒ–å‚æ•° - å¢åŠ åˆå§‹æ¢ç´¢
        self.INITIAL_SEARCH = 10
        self.OPT_SEARCH = 5
        self.NOISE_SAMPLES = 3  # å¤šæ¬¡é‡‡æ ·å–å¹³å‡
        self.EARLY_STOP_SCORE = 1000
        self.ALPHA = 1e-2
        
        # æ¨¡æ‹Ÿå™ªå£°ï¼ˆä¸ BasicAgentPro ä¿æŒä¸€è‡´ï¼‰
        self.noise_std = {
            'V0': 0.1,
            'phi': 0.15,
            'theta': 0.1,
            'a': 0.005,
            'b': 0.005
        }
        self.enable_noise = enable_noise
        
        logger.info("[BayesMCTS] (Enhanced v2) å·²åˆå§‹åŒ–ã€‚")
    
    def _create_optimizer(self, reward_function, seed):
        """åˆ›å»ºè´å¶æ–¯ä¼˜åŒ–å™¨
        
        å‚æ•°ï¼š
            reward_function: ç›®æ ‡å‡½æ•°ï¼Œ(V0, phi, theta, a, b) -> score
            seed: éšæœºç§å­
        
        è¿”å›ï¼š
            BayesianOptimizationå¯¹è±¡
        """
        gpr = GaussianProcessRegressor(
            kernel=Matern(nu=2.5),
            alpha=self.ALPHA,
            n_restarts_optimizer=10,
            random_state=seed
        )
        
        bounds_transformer = SequentialDomainReductionTransformer(
            gamma_osc=0.8,
            gamma_pan=1.0
        )
        
        optimizer = BayesianOptimization(
            f=reward_function,
            pbounds=self.pbounds,
            random_state=seed,
            verbose=0,
            bounds_transformer=bounds_transformer
        )
        optimizer._gp = gpr
        
        return optimizer
    
    def _evaluate_action(self, d_V0, d_phi, theta, a, b, base_phi, base_v, balls, table, my_targets, last_state_snapshot):
        """
        å¸¦å¤šæ¬¡å™ªå£°é‡‡æ ·çš„åŠ¨ä½œè¯„ä¼° (æ ¸å¿ƒæ”¹è¿›)
        
        æ”¹è¿›ç‚¹ï¼š
        1. å¤šæ¬¡é‡‡æ ·å–å¹³å‡ï¼Œæé«˜ç¨³å¥æ€§ï¼ˆä¸MCTSæ€æƒ³å¯¹é½ï¼‰
        2. ä½¿ç”¨ analyze_shot_for_rewardï¼ˆä¸ BasicAgentPro å¯¹é½ï¼‰
        """
        # 1. è¿˜åŸç»å¯¹å‚æ•°
        phi_base = (base_phi + d_phi) % 360
        V0_base = np.clip(base_v + d_V0, 0.8, 7.5)
        
        # 2. å¤šæ¬¡å™ªå£°é‡‡æ ·
        n_samples = self.NOISE_SAMPLES if self.enable_noise else 1
        scores = []
        
        for _ in range(n_samples):
            # æ³¨å…¥å™ªå£°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.enable_noise:
                V0 = np.clip(V0_base + np.random.normal(0, self.noise_std['V0']), 0.5, 8.0)
                phi = (phi_base + np.random.normal(0, self.noise_std['phi'])) % 360
                theta_n = np.clip(theta + np.random.normal(0, self.noise_std['theta']), 0, 90)
                a_n = np.clip(a + np.random.normal(0, self.noise_std['a']), -0.5, 0.5)
                b_n = np.clip(b + np.random.normal(0, self.noise_std['b']), -0.5, 0.5)
            else:
                V0, phi, theta_n, a_n, b_n = V0_base, phi_base, theta, a, b
            
            # æ„å»ºæ¨¡æ‹Ÿç¯å¢ƒ
            sim_balls = {bid: copy.deepcopy(ball) for bid, ball in balls.items()}
            sim_table = copy.deepcopy(table)
            cue = pt.Cue(cue_ball_id="cue")
            sim_shot = pt.System(table=sim_table, balls=sim_balls, cue=cue)
            sim_shot.cue.set_state(V0=V0, phi=phi, theta=theta_n, a=a_n, b=b_n)
            
            try:
                pt.simulate(sim_shot, inplace=True)
                # ä½¿ç”¨ä¸ BasicAgentPro ç›¸åŒçš„è¯„ä¼°å‡½æ•°ï¼
                score = evaluate_state(sim_shot, last_state_snapshot, my_targets)
            except:
                score = -500.0
            
            scores.append(score)
        
        # è¿”å›å¹³å‡åˆ†ï¼ˆæ›´ç¨³å¥ï¼‰
        return np.mean(scores)

    def decision(self, balls=None, my_targets=None, table=None):
        """ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–æœç´¢æœ€ä½³å‡»çƒå‚æ•°
        
        å‚æ•°ï¼š
            balls: çƒçŠ¶æ€å­—å…¸ï¼Œ{ball_id: Ball}
            my_targets: ç›®æ ‡çƒIDåˆ—è¡¨ï¼Œ['1', '2', ...]
            table: çƒæ¡Œå¯¹è±¡
        
        è¿”å›ï¼š
            dict: å‡»çƒåŠ¨ä½œ {'V0', 'phi', 'theta', 'a', 'b'}
                å¤±è´¥æ—¶è¿”å›éšæœºåŠ¨ä½œ
        """
        if balls is None:
            logger.info("[BayesMCTS] Agent decisionå‡½æ•°æœªæ”¶åˆ°ballså…³é”®ä¿¡æ¯ï¼Œä½¿ç”¨éšæœºåŠ¨ä½œã€‚")
            return self._random_action()
        try:
            
            # ä¿å­˜ä¸€ä¸ªå‡»çƒå‰çš„çŠ¶æ€å¿«ç…§ï¼Œç”¨äºå¯¹æ¯”
            last_state_snapshot = {bid: copy.deepcopy(ball) for bid, ball in balls.items()}

            remaining_own = [bid for bid in my_targets if balls[bid].state.s != 4]
            if len(remaining_own) == 0:
                remaining_own = ["8"]
                logger.info("[BayesMCTS] æˆ‘çš„ç›®æ ‡çƒå·²å…¨éƒ¨æ¸…ç©ºï¼Œè‡ªåŠ¨åˆ‡æ¢ç›®æ ‡ä¸ºï¼š8å·çƒ")
            
            cue_ball = balls['cue']
            cue_pos = cue_ball.state.rvw[0]
            R = cue_ball.params.R

            # ========== å¼€çƒç‰¹æ®Šå¤„ç† ==========
            # åˆ¤æ–­æ˜¯å¦ä¸ºå¼€çƒå±€é¢ï¼šæ‰€æœ‰ç›®æ ‡çƒéƒ½åœ¨å°ä¸Šï¼ˆ7ä¸ªï¼‰
            is_break_shot = balls['1'].state.t == 0
            
            if is_break_shot:
                # å¼€çƒç­–ç•¥ï¼šç›´æ¥å¤§åŠ›å†²å‡»çƒå †ï¼Œä¸åšå¤æ‚æœç´¢
                # æ ‡å‡†å¼€çƒè§’åº¦ï¼šç„å‡†1å·çƒï¼ˆçƒå †é¡¶ç«¯ï¼‰
                one_pos = balls['1'].state.rvw[0]
                dx = one_pos[0] - cue_pos[0]
                dy = one_pos[1] - cue_pos[1]
                phi_break = np.degrees(np.arctan2(dy, dx)) % 360
                
                logger.info("[BayesMCTS] æ£€æµ‹åˆ°å¼€çƒå±€é¢ï¼Œä½¿ç”¨å¿«é€Ÿå¼€çƒç­–ç•¥ (phi=%.1f)", phi_break)
                return {
                    'V0': 7.0,  # å¤§åŠ›å¼€çƒ
                    'phi': phi_break,
                    'theta': 0,
                    'a': 0,
                    'b': 0
                }
            # ====================================
            
            candidates = []

            logger.info("[BayesMCTS] æ­£åœ¨ä¸º Player (targets: %s) æœç´¢æœ€ä½³å‡»çƒ...", remaining_own)
            for ball_id in remaining_own:
                obj_pos = balls[ball_id].state.rvw[0]
                for pid, pocket in table.pockets.items():
                    phi_ideal, cut_angle, dist = calculate_ghost_ball_params(cue_pos, obj_pos, pocket.center, R)
                    
                    # æ”¾å®½é˜ˆå€¼åˆ° 80 åº¦ï¼ˆä¸å…¶ä»– Agent ä¸€è‡´ï¼‰
                    if abs(cut_angle) > 80: continue
                    
                    candidates.append({
                        'target_id': ball_id,
                        'phi_center': phi_ideal,
                        'cut_angle': cut_angle,
                        'distance': dist
                    })
            
            # æ”¹è¿›æ’åºï¼šé™ä½è·ç¦»æƒé‡ï¼Œä¼˜å…ˆè€ƒè™‘åˆ‡è§’
            # åŸå…¬å¼ cut_angle + 10*dist å¯¹è¿œå°æƒ©ç½šè¿‡é‡
            candidates.sort(key=lambda x: x['cut_angle'] * 1.5 + x['distance'] * 5)
            
            # åªå–å‰ 3 ä¸ªå€™é€‰ï¼Œå¹³è¡¡é€Ÿåº¦ä¸è´¨é‡
            top_candidates = candidates[:3]

            top_action = None
            top_score = -float('inf')
            top_base_phi = 0
            top_base_v = 0
            
            # æ—©åœæ ‡å¿—
            found_good_shot = False

            for cand in top_candidates:
                # æ—©åœï¼šå·²æ‰¾åˆ°è¿›çƒæ–¹æ¡ˆï¼Œä¸å†ç»§ç»­æœç´¢
                if found_good_shot:
                    break
                    
                # 1.åŠ¨æ€åˆ›å»ºâ€œå¥–åŠ±å‡½æ•°â€ (Wrapper)
                # è´å¶æ–¯ä¼˜åŒ–å™¨ä¼šè°ƒç”¨æ­¤å‡½æ•°ï¼Œå¹¶ä¼ å…¥å‚æ•°

                # å‡ ä½•å…ˆéªŒè§’åº¦
                base_phi = cand['phi_center']
                # ç²—ç•¥ä¼°è®¡é€Ÿåº¦
                base_v = 1.5 + cand['distance'] * 1.5
                
                # ä½¿ç”¨ partial ç»‘å®šå‚æ•°ï¼Œç¡®ä¿å˜é‡éš”ç¦»ï¼
                # è¿™æ ·ä¼˜åŒ–å™¨è°ƒç”¨çš„å‡½æ•°å°±åªå‰© (V0, d_phi, theta, a, b) è¿™5ä¸ªå‚æ•°äº†
                target_func = functools.partial(
                    self._evaluate_action,
                    base_phi=base_phi,       # ç»‘å®šå½“å‰çš„å‡ ä½•è§’
                    base_v=base_v,           # ç»‘å®šå½“å‰çš„ä¼°è®¡é€Ÿåº¦
                    balls=balls,             # ç»‘å®šå½“å‰çƒçŠ¶æ€
                    table=table,
                    my_targets=remaining_own,
                    last_state_snapshot=last_state_snapshot
                )
                
                # åˆ›å»ºä¼˜åŒ–å™¨
                seed = np.random.randint(1e6)
                optimizer = self._create_optimizer(target_func, seed)
                optimizer.maximize(
                    init_points=self.INITIAL_SEARCH,
                    n_iter=self.OPT_SEARCH
                )
                
                best_result = optimizer.max
                best_params = best_result['params']
                best_score = best_result['target']

                final_phi = (float(best_params['d_phi']) + base_phi) % 360
                final_v = np.clip((float(best_params['d_V0']) + base_v), 0.8, 7.5)

                action = {
                    'V0': final_v,
                    'phi': final_phi,
                    'theta': float(best_params['theta']),
                    'a': float(best_params['a']),
                    'b': float(best_params['b']),
                }

                if best_score > top_score:
                    top_score = best_score
                    top_action = action
                    top_base_phi = base_phi
                    top_base_v = base_v
                    
                    # æ—©åœï¼šæ‰¾åˆ°è¿›çƒæ–¹æ¡ˆåä¸å†æœç´¢å…¶ä»–å€™é€‰
                    if best_score >= self.EARLY_STOP_SCORE:
                        logger.info("[BayesMCTS] æ—©åœï¼šæ‰¾åˆ°è¿›çƒæ–¹æ¡ˆ (score=%.1f)", best_score)
                        found_good_shot = True

            if top_score < 0:  # å‡å°‘è¯¯æ‰“é»‘8
                logger.info("[BayesMCTS] æœªæ‰¾åˆ°å¥½çš„æ–¹æ¡ˆ (æœ€é«˜åˆ†: %.2f)ã€‚ä½¿ç”¨éšæœºåŠ¨ä½œã€‚", top_score)
                return self._random_action()
            
            logger.info(
                "[BayesMCTS] å†³ç­– (å¾—åˆ†: %.2f): V0=%.2f, V0_base=%.2f, phi=%.2f, phi_base=%.2f Î¸=%.2f, a=%.3f, b=%.3f",
                top_score,
                top_action['V0'],
                top_base_v,
                top_action['phi'],
                top_base_phi,
                top_action['theta'],
                top_action['a'],
                top_action['b'],
            )
            return top_action

        except Exception as e:
            logger.error("[BayesMCTS] å†³ç­–æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯ï¼Œä½¿ç”¨éšæœºåŠ¨ä½œã€‚åŸå› : %s", e)
            import traceback
            logger.exception(e)
            return self._random_action()
